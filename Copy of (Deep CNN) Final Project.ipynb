{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of (Deep CNN) Final Project.ipynb","provenance":[{"file_id":"1wBjPUfYLxtrxUd3mnHqMmYqZreYKing2","timestamp":1571308388566},{"file_id":"1MmKjgah6-xSUwCB_A77hXljE-vTAxJ5N","timestamp":1570858985822}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SE0AaUB0YaO4","colab_type":"code","outputId":"2542ca7f-2d2e-4a73-c963-0f0f306d8586","executionInfo":{"status":"ok","timestamp":1571541113794,"user_tz":-660,"elapsed":2272,"user":{"displayName":"Ba Nguyen","photoUrl":"","userId":"04061138511611591689"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","import cv2\n","import copy\n","##Checking for GPU: \n","device = torch.device(\"cuda\")\n","print(torch.cuda.get_device_name(0))\n","import seaborn as sns\n","import pylab\n","import gzip\n","import pickle\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"07kRI6muc2os","colab_type":"code","outputId":"6152224e-311a-434d-80a1-a0d6a49a68d0","executionInfo":{"status":"ok","timestamp":1571541113795,"user_tz":-660,"elapsed":2261,"user":{"displayName":"Ba Nguyen","photoUrl":"","userId":"04061138511611591689"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","import sys\n","\n","drive.mount('/content/gdrive')\n","sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")\n","ROOT = \"/content/gdrive/My Drive/Colab Notebooks\"\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cuKoqX0zEAIr","colab_type":"code","outputId":"bb4fc61f-ecbe-4a00-bb3d-409167f5dbca","executionInfo":{"status":"ok","timestamp":1571541159246,"user_tz":-660,"elapsed":47701,"user":{"displayName":"Ba Nguyen","photoUrl":"","userId":"04061138511611591689"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["transform_train = transforms.Compose([\n","    transforms.ToTensor(),\n","    torchvision.transforms.ToPILImage(mode=None),\n","    torchvision.transforms.Resize((96,96), interpolation=2),\n","    transforms.ToTensor()\n","])\n","\n","\n","##DOWNLOADING VOC DATA SET: DETECTION:\n","training2 = torchvision.datasets.VOCDetection(ROOT +\"/VOC\", year='2012', image_set='trainval', download=True, transform=transform_train)\n","test2 = torchvision.datasets.VOCDetection(ROOT +\"/VOC\", year='2012', image_set='val', download=True, transform=transform_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using downloaded and verified file: /content/gdrive/My Drive/Colab Notebooks/VOC/VOCtrainval_11-May-2012.tar\n","Using downloaded and verified file: /content/gdrive/My Drive/Colab Notebooks/VOC/VOCtrainval_11-May-2012.tar\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tpir5T7q9vsD","colab_type":"code","colab":{}},"source":["##TRAINING DATA SET: \n","images_vehicle = torch.zeros(3542,3,96,96)\n","labels_name = []\n","bound_box_array = []\n","bound_box_temp  = [] \n","j = 0\n","\n","##Extracting Vehicles from the VOC dataset\n","for i in range(len(training2)):\n","  image, image_label = training2[i]\n","  if (type(image_label['annotation']['object']) is list):\n","      check = image_label['annotation']['object'][0]['name']\n","      if (check == 'bicycle' or check == 'boat' or check == 'bus' or check == 'car' or check == 'motorbike' or check == 'train' or check == 'aeroplane'):\n","        bound_box_temp.append(int(image_label['annotation']['object'][0]['bndbox']['xmin']))\n","        bound_box_temp.append(int(image_label['annotation']['object'][0]['bndbox']['xmax']))\n","        bound_box_temp.append(int(image_label['annotation']['object'][0]['bndbox']['ymin']))\n","        bound_box_temp.append(int(image_label['annotation']['object'][0]['bndbox']['ymax']))\n","        bound_box_array.append(bound_box_temp)\n","        bound_box_temp = [] \n","        labels_name.append(check)\n","        images_vehicle[j] = image\n","        j = j + 1\n","  elif(type(image_label['annotation']['object']) is dict):\n","      check2 = image_label['annotation']['object']['name']\n","      if (check2 == 'bicycle' or check2 == 'boat' or check2 == 'bus' or check2 == 'car' or check2 == 'motorbike' or check2 == 'train' or check2 == 'aeroplane'):\n","        bound_box_temp.append(int(image_label['annotation']['object']['bndbox']['xmin']))\n","        bound_box_temp.append(int(image_label['annotation']['object']['bndbox']['xmax']))\n","        bound_box_temp.append(int(image_label['annotation']['object']['bndbox']['ymin']))\n","        bound_box_temp.append(int(image_label['annotation']['object']['bndbox']['ymax']))\n","        bound_box_array.append(bound_box_temp)\n","        bound_box_temp = [] \n","        labels_name.append(check2)\n","        images_vehicle[j] = image\n","        j = j + 1\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9h6u4dv9zcD","colab_type":"code","colab":{}},"source":["##TESTING DATA SET: \n","images_vehicle_TEST = torch.zeros(1766,3,96,96)\n","labels_name_test = []\n","bound_box_array_test = []\n","bound_box_temp_test  = [] \n","j = 0\n","\n","##Extracting Vehicles from the VOC dataset\n","for i in range(len(test2)):\n","  image, image_label = test2[i]\n","  if (type(image_label['annotation']['object']) is list):\n","      check = image_label['annotation']['object'][0]['name']\n","      if (check == 'bicycle' or check == 'boat' or check == 'bus' or check == 'car' or check == 'motorbike' or check == 'train' or check == 'aeroplane'):\n","        bound_box_temp_test.append(int(image_label['annotation']['object'][0]['bndbox']['xmin']))\n","        bound_box_temp_test.append(int(image_label['annotation']['object'][0]['bndbox']['xmax']))\n","        bound_box_temp_test.append(int(image_label['annotation']['object'][0]['bndbox']['ymin']))\n","        bound_box_temp_test.append(int(image_label['annotation']['object'][0]['bndbox']['ymax']))\n","        bound_box_array_test.append(bound_box_temp_test)\n","        bound_box_temp_test = [] \n","        labels_name_test.append(check)\n","        images_vehicle_TEST[j] = image\n","        j = j + 1\n","  elif(type(image_label['annotation']['object']) is dict):\n","      check2 = image_label['annotation']['object']['name']\n","      if (check2 == 'bicycle' or check2 == 'boat' or check2 == 'bus' or check2 == 'car' or check2 == 'motorbike' or check2 == 'train' or check2 == 'aeroplane'):\n","        bound_box_temp_test.append(int(image_label['annotation']['object']['bndbox']['xmin']))\n","        bound_box_temp_test.append(int(image_label['annotation']['object']['bndbox']['xmax']))\n","        bound_box_temp_test.append(int(image_label['annotation']['object']['bndbox']['ymin']))\n","        bound_box_temp_test.append(int(image_label['annotation']['object']['bndbox']['ymax']))\n","        bound_box_array_test.append(bound_box_temp)\n","        bound_box_temp_test = [] \n","        labels_name_test.append(check2)\n","        images_vehicle_TEST[j] = image\n","        j = j + 1\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpoKIOHj973E","colab_type":"text"},"source":["**Convert labels name to number** "]},{"cell_type":"code","metadata":{"id":"Od6_avQQ97AJ","colab_type":"code","colab":{}},"source":["def name2num(labels_name):\n","  label_num2 = []\n","  \n","  for i in range(len(labels_name)):\n","    if(labels_name[i] == 'car'):\n","      label_num2.append(0)\n","    elif(labels_name[i] == 'aeroplane'):\n","      label_num2.append(1)\n","    elif(labels_name[i] == 'bicycle'):\n","      label_num2.append(2)\n","    elif(labels_name[i] == 'train'):\n","      label_num2.append(3)\n","    elif(labels_name[i] == 'bus'):\n","      label_num2.append(4)\n","    elif(labels_name[i] == 'motorbike'):\n","      label_num2.append(5)\n","    elif(labels_name[i] == 'boat'):\n","      label_num2.append(6)    \n","  return label_num2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uw2Fz29M-srM","colab_type":"code","colab":{}},"source":["label_num = name2num(labels_name)\n","label_num2 = name2num(labels_name_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wncEkH_MMAcf","colab_type":"code","colab":{}},"source":["\n","\n","def bbox_to_rect(bbox, color):\n","\n","    return plt.Rectangle(\n","        xy=(bbox[0], bbox[2]), width=bbox[1]-bbox[0], height=bbox[3]-bbox[2],\n","        fill=False, edgecolor=color, linewidth=2)\n","  \n","\n","class VOC(Dataset):\n","    def __init__(self,trn_val_tst = 0, transform=None):\n","        if trn_val_tst == 0:\n","            #trainloader\n","            self.images = images_vehicle\n","            self.labels = label_num\n","        else:\n","            #testloader\n","            self.images = images_vehicle_TEST\n","            self.labels = label_num2\n","            \n","        self.images = np.float32(self.images)/1.0\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","   \n","        sample = self.images[idx,:]\n","        labels = self.labels[idx]\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample, labels\n","      \n","      \n","transform_train = transforms.Compose([\n","    transforms.ToTensor(),\n","    torchvision.transforms.ToPILImage(mode=None),\n","    transforms.RandomResizedCrop(32),\n","    transforms.RandomRotation(10),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])      \n","      \n","      \n","train_set = VOC(trn_val_tst=0, transform = None)\n","train_set_augment = VOC(trn_val_tst=0, transform = transform_train) \n","test_set = VOC(trn_val_tst=1, transform = None)\n","\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_set, \n","                                           batch_size=100, \n","                                           shuffle=True)\n","\n","train_loader_real = torch.utils.data.DataLoader(dataset=train_set, \n","                                           batch_size=100, \n","                                           shuffle=True)\n","\n","train_loader_aug = torch.utils.data.DataLoader(dataset=train_set_augment, \n","                                           batch_size=100, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_set, \n","                                           batch_size=100, \n","                                           shuffle=False)\n","\n","\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9y28VM98HuD","colab_type":"text"},"source":["**Confusion Matrix**"]},{"cell_type":"code","metadata":{"id":"E-GgcqmS8HJL","colab_type":"code","colab":{}},"source":["def confusion(actual,assign):\n","  confusion_arr = torch.zeros(7,7)\n","  for j in range(len(actual)):\n","    for i in range(len(actual[j])):\n","      confusion_arr[actual[j][i]][assign[j][i]] += 1\n","    \n","  return confusion_arr"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4As5ZRP8PAj","colab_type":"text"},"source":["**Model Clarification**"]},{"cell_type":"code","metadata":{"id":"WKyBKimrtobX","colab_type":"code","colab":{}},"source":["# Model for training\n","class DeepLearning(nn.Module):\n","    def __init__(self):\n","        # Call the __init__ function of the parent nn.module class\n","        super(DeepLearning, self).__init__()\n","        \n","        \n","        self.conv_layer = nn.Sequential(\n","\n","            # Conv Layer block 1\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32,64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2,2),\n","\n","            # Conv Layer block 2\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.Dropout2d(p=0.05),\n","\n","            # Conv Layer block 3\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2,2)\n","        )\n","        \n","        ##GAP Layer: \n","        self.GAP = nn.AdaptiveAvgPool2d(1)\n","        \n","        ##Fully Connected Layers: \n","        self.fc_layer = nn.Sequential(\n","            nn.Dropout(p=0.1),\n","            nn.Linear(256, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.1),\n","            nn.Linear(512, 7)\n","        )\n","        \n","    ## Performing Foward Propogation: \n","    def forward(self, x):\n","        x = self.conv_layer(x)\n","        x = self.GAP(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc_layer(x)\n","        return x "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wrGqd5LfmVL","colab_type":"code","colab":{}},"source":["# Model for training\n","class ShallowLearning(nn.Module):\n","    def __init__(self):\n","        # Call the __init__ function of the parent nn.module class\n","        super(ShallowLearning, self).__init__()\n","        \n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0),\n","            nn.ReLU())\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),\n","            nn.ReLU())\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),\n","            nn.ReLU())\n","        self.maxpool =  nn.MaxPool2d(kernel_size=3, stride = 3)\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(1152, 128),\n","            nn.ReLU())\n","        self.fc2 = nn.Linear(128,7)\n","        \n","        \n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.maxpool(out)\n","        out = out.reshape(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0QM9SrMO8T_G","colab_type":"text"},"source":["**Training and Validating Data**"]},{"cell_type":"code","metadata":{"id":"sZEI1i7WhJ54","colab_type":"code","colab":{}},"source":["# model = ShallowLearning()\n","model = DeepLearning()\n","# model = torchvision.models.resnet101(pretrained=False, progress=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmQskMW1sGuc","colab_type":"code","outputId":"c24a9c08-2b49-4081-c5c0-1d87de8f902e","executionInfo":{"status":"ok","timestamp":1571542811200,"user_tz":-660,"elapsed":1010015,"user":{"displayName":"Ba Nguyen","photoUrl":"","userId":"04061138511611591689"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["##Defining Step Size:  \n","total_step = len(train_loader)\n","total_step_val = len(test_loader)\n","softmax = nn.Softmax(dim=1)\n","\n","\n","num_epochs = 60\n","learning_rate = 0.0001\n","\n","\n","model.cuda()\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","loss_list = []\n","loss2_list = []\n","acc_list = []\n","acc2_list = []\n","acc3_list = [] \n","\n","\n","for epoch in range(num_epochs):\n","    total = 0\n","    correct = 0\n","    model.train()\n","    #Train the neural network using Augmented Data Set: \n","    if (epoch%2==0):\n","      train_loader = train_loader_aug\n","    else:\n","      ##Train the neural network using the Original Data Set: \n","      train_loader = train_loader\n","    for i, (images, labels) in enumerate(train_loader_real):\n","          \n","    ##Training \n","        images = images.to(device)\n","        labels = labels.long()\n","        labels = labels.to(device)\n","   \n","            \n","        # Run the forward pass (Training):\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","       \n","\n","        \n","        # Backprop and perform Adam optimisation\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted== labels.long()).sum().item()\n","       \n","        if (i+1) % total_step == 0:\n","            loss_list.append(loss.item())\n","            acc_list.append(correct / total)\n","            print('Epoch [ {}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                          (correct / total) * 100))\n","          \n","        if (i+1) % total_step == 0:  \n","            model.eval()\n","            with torch.no_grad():\n","              correct_val = 0\n","              total_val = 0\n","              for j, (images_valid, labels_valid) in enumerate(test_loader):\n","                  images_valid = images_valid.to(device)\n","                  labels_valid = labels_valid.to(device)\n","                  outputs2 = model(images_valid)\n","                  loss2 = criterion(outputs2, labels_valid.long())\n","                  _, predicted_valid = torch.max(outputs2.data, 1)\n","                  total_val += labels_valid.size(0)\n","                  correct_val += (predicted_valid == labels_valid.long()).sum().item()\n","               \n","                  if (j+1) % total_step_val == 0:\n","                    loss2_list.append(loss2.item())\n","                    acc2_list.append(correct_val / total_val)\n","                    print('Epoch [ {}/{}], Step_val [{}/{}], Loss_val: {:.4f}, Accuracy_val: {:.2f}%'\n","                  .format(epoch + 1, num_epochs, j + 1, total_step_val, loss2.item(),\n","                          (correct_val / total_val) * 100))\n","                    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch [ 1/60], Step [36/36], Loss: 1.5894, Accuracy: 28.66%\n","Epoch [ 1/60], Step_val [18/18], Loss_val: 2.0806, Accuracy_val: 18.46%\n","Epoch [ 2/60], Step [36/36], Loss: 1.7574, Accuracy: 32.41%\n","Epoch [ 2/60], Step_val [18/18], Loss_val: 1.5861, Accuracy_val: 34.48%\n","Epoch [ 3/60], Step [36/36], Loss: 1.4294, Accuracy: 35.88%\n","Epoch [ 3/60], Step_val [18/18], Loss_val: 1.5718, Accuracy_val: 35.05%\n","Epoch [ 4/60], Step [36/36], Loss: 1.5840, Accuracy: 38.26%\n","Epoch [ 4/60], Step_val [18/18], Loss_val: 1.5911, Accuracy_val: 41.90%\n","Epoch [ 5/60], Step [36/36], Loss: 1.6568, Accuracy: 41.36%\n","Epoch [ 5/60], Step_val [18/18], Loss_val: 1.4750, Accuracy_val: 44.51%\n","Epoch [ 6/60], Step [36/36], Loss: 1.5350, Accuracy: 43.79%\n","Epoch [ 6/60], Step_val [18/18], Loss_val: 1.3880, Accuracy_val: 47.45%\n","Epoch [ 7/60], Step [36/36], Loss: 1.5471, Accuracy: 46.10%\n","Epoch [ 7/60], Step_val [18/18], Loss_val: 1.5074, Accuracy_val: 44.68%\n","Epoch [ 8/60], Step [36/36], Loss: 1.3347, Accuracy: 49.44%\n","Epoch [ 8/60], Step_val [18/18], Loss_val: 1.3544, Accuracy_val: 47.40%\n","Epoch [ 9/60], Step [36/36], Loss: 1.5043, Accuracy: 51.89%\n","Epoch [ 9/60], Step_val [18/18], Loss_val: 1.5999, Accuracy_val: 44.56%\n","Epoch [ 10/60], Step [36/36], Loss: 1.4238, Accuracy: 53.47%\n","Epoch [ 10/60], Step_val [18/18], Loss_val: 1.2530, Accuracy_val: 54.70%\n","Epoch [ 11/60], Step [36/36], Loss: 1.2026, Accuracy: 55.03%\n","Epoch [ 11/60], Step_val [18/18], Loss_val: 1.3123, Accuracy_val: 53.79%\n","Epoch [ 12/60], Step [36/36], Loss: 1.2458, Accuracy: 56.27%\n","Epoch [ 12/60], Step_val [18/18], Loss_val: 1.2026, Accuracy_val: 53.62%\n","Epoch [ 13/60], Step [36/36], Loss: 1.2200, Accuracy: 58.36%\n","Epoch [ 13/60], Step_val [18/18], Loss_val: 1.1745, Accuracy_val: 59.97%\n","Epoch [ 14/60], Step [36/36], Loss: 0.8329, Accuracy: 60.42%\n","Epoch [ 14/60], Step_val [18/18], Loss_val: 1.3165, Accuracy_val: 57.02%\n","Epoch [ 15/60], Step [36/36], Loss: 0.9644, Accuracy: 60.76%\n","Epoch [ 15/60], Step_val [18/18], Loss_val: 1.1562, Accuracy_val: 63.70%\n","Epoch [ 16/60], Step [36/36], Loss: 1.2442, Accuracy: 61.43%\n","Epoch [ 16/60], Step_val [18/18], Loss_val: 1.1438, Accuracy_val: 50.68%\n","Epoch [ 17/60], Step [36/36], Loss: 1.0278, Accuracy: 62.93%\n","Epoch [ 17/60], Step_val [18/18], Loss_val: 1.0071, Accuracy_val: 59.91%\n","Epoch [ 18/60], Step [36/36], Loss: 1.0518, Accuracy: 63.30%\n","Epoch [ 18/60], Step_val [18/18], Loss_val: 1.2701, Accuracy_val: 57.47%\n","Epoch [ 19/60], Step [36/36], Loss: 0.9425, Accuracy: 66.49%\n","Epoch [ 19/60], Step_val [18/18], Loss_val: 0.9802, Accuracy_val: 68.29%\n","Epoch [ 20/60], Step [36/36], Loss: 0.6740, Accuracy: 67.53%\n","Epoch [ 20/60], Step_val [18/18], Loss_val: 1.1577, Accuracy_val: 64.95%\n","Epoch [ 21/60], Step [36/36], Loss: 0.5639, Accuracy: 68.27%\n","Epoch [ 21/60], Step_val [18/18], Loss_val: 0.9282, Accuracy_val: 61.27%\n","Epoch [ 22/60], Step [36/36], Loss: 0.9465, Accuracy: 68.55%\n","Epoch [ 22/60], Step_val [18/18], Loss_val: 1.0420, Accuracy_val: 66.65%\n","Epoch [ 23/60], Step [36/36], Loss: 0.6471, Accuracy: 68.58%\n","Epoch [ 23/60], Step_val [18/18], Loss_val: 1.4079, Accuracy_val: 54.02%\n","Epoch [ 24/60], Step [36/36], Loss: 0.8633, Accuracy: 71.15%\n","Epoch [ 24/60], Step_val [18/18], Loss_val: 1.1918, Accuracy_val: 68.29%\n","Epoch [ 25/60], Step [36/36], Loss: 0.9050, Accuracy: 73.01%\n","Epoch [ 25/60], Step_val [18/18], Loss_val: 0.7341, Accuracy_val: 72.99%\n","Epoch [ 26/60], Step [36/36], Loss: 0.7305, Accuracy: 73.83%\n","Epoch [ 26/60], Step_val [18/18], Loss_val: 1.0256, Accuracy_val: 71.29%\n","Epoch [ 27/60], Step [36/36], Loss: 0.5431, Accuracy: 73.88%\n","Epoch [ 27/60], Step_val [18/18], Loss_val: 0.7956, Accuracy_val: 77.29%\n","Epoch [ 28/60], Step [36/36], Loss: 0.8898, Accuracy: 76.34%\n","Epoch [ 28/60], Step_val [18/18], Loss_val: 0.8083, Accuracy_val: 77.58%\n","Epoch [ 29/60], Step [36/36], Loss: 0.7738, Accuracy: 77.53%\n","Epoch [ 29/60], Step_val [18/18], Loss_val: 0.6470, Accuracy_val: 80.07%\n","Epoch [ 30/60], Step [36/36], Loss: 0.5909, Accuracy: 79.93%\n","Epoch [ 30/60], Step_val [18/18], Loss_val: 0.5665, Accuracy_val: 81.03%\n","Epoch [ 31/60], Step [36/36], Loss: 0.5691, Accuracy: 77.95%\n","Epoch [ 31/60], Step_val [18/18], Loss_val: 0.9303, Accuracy_val: 66.82%\n","Epoch [ 32/60], Step [36/36], Loss: 0.5028, Accuracy: 79.79%\n","Epoch [ 32/60], Step_val [18/18], Loss_val: 0.7993, Accuracy_val: 66.42%\n","Epoch [ 33/60], Step [36/36], Loss: 0.3628, Accuracy: 82.35%\n","Epoch [ 33/60], Step_val [18/18], Loss_val: 0.6251, Accuracy_val: 81.94%\n","Epoch [ 34/60], Step [36/36], Loss: 0.4959, Accuracy: 85.60%\n","Epoch [ 34/60], Step_val [18/18], Loss_val: 0.3652, Accuracy_val: 83.07%\n","Epoch [ 35/60], Step [36/36], Loss: 0.5998, Accuracy: 84.25%\n","Epoch [ 35/60], Step_val [18/18], Loss_val: 0.5764, Accuracy_val: 79.28%\n","Epoch [ 36/60], Step [36/36], Loss: 0.2588, Accuracy: 85.86%\n","Epoch [ 36/60], Step_val [18/18], Loss_val: 0.4116, Accuracy_val: 85.28%\n","Epoch [ 37/60], Step [36/36], Loss: 0.2005, Accuracy: 86.84%\n","Epoch [ 37/60], Step_val [18/18], Loss_val: 0.6148, Accuracy_val: 80.18%\n","Epoch [ 38/60], Step [36/36], Loss: 0.3993, Accuracy: 87.58%\n","Epoch [ 38/60], Step_val [18/18], Loss_val: 0.2337, Accuracy_val: 91.39%\n","Epoch [ 39/60], Step [36/36], Loss: 0.2230, Accuracy: 90.32%\n","Epoch [ 39/60], Step_val [18/18], Loss_val: 0.5596, Accuracy_val: 84.77%\n","Epoch [ 40/60], Step [36/36], Loss: 0.3137, Accuracy: 90.43%\n","Epoch [ 40/60], Step_val [18/18], Loss_val: 0.7402, Accuracy_val: 79.22%\n","Epoch [ 41/60], Step [36/36], Loss: 0.2263, Accuracy: 90.94%\n","Epoch [ 41/60], Step_val [18/18], Loss_val: 0.4016, Accuracy_val: 83.41%\n","Epoch [ 42/60], Step [36/36], Loss: 0.2483, Accuracy: 92.46%\n","Epoch [ 42/60], Step_val [18/18], Loss_val: 0.2296, Accuracy_val: 92.41%\n","Epoch [ 43/60], Step [36/36], Loss: 0.2383, Accuracy: 91.93%\n","Epoch [ 43/60], Step_val [18/18], Loss_val: 1.4486, Accuracy_val: 71.69%\n","Epoch [ 44/60], Step [36/36], Loss: 0.2551, Accuracy: 90.97%\n","Epoch [ 44/60], Step_val [18/18], Loss_val: 0.4707, Accuracy_val: 85.11%\n","Epoch [ 45/60], Step [36/36], Loss: 0.2509, Accuracy: 91.36%\n","Epoch [ 45/60], Step_val [18/18], Loss_val: 0.4376, Accuracy_val: 86.92%\n","Epoch [ 46/60], Step [36/36], Loss: 0.2161, Accuracy: 93.45%\n","Epoch [ 46/60], Step_val [18/18], Loss_val: 0.1883, Accuracy_val: 90.26%\n","Epoch [ 47/60], Step [36/36], Loss: 0.2713, Accuracy: 94.58%\n","Epoch [ 47/60], Step_val [18/18], Loss_val: 0.2270, Accuracy_val: 91.51%\n","Epoch [ 48/60], Step [36/36], Loss: 0.4653, Accuracy: 94.47%\n","Epoch [ 48/60], Step_val [18/18], Loss_val: 0.1339, Accuracy_val: 95.53%\n","Epoch [ 49/60], Step [36/36], Loss: 0.1533, Accuracy: 95.12%\n","Epoch [ 49/60], Step_val [18/18], Loss_val: 0.1171, Accuracy_val: 96.32%\n","Epoch [ 50/60], Step [36/36], Loss: 0.1014, Accuracy: 96.84%\n","Epoch [ 50/60], Step_val [18/18], Loss_val: 0.0869, Accuracy_val: 97.79%\n","Epoch [ 51/60], Step [36/36], Loss: 0.1172, Accuracy: 95.12%\n","Epoch [ 51/60], Step_val [18/18], Loss_val: 0.0235, Accuracy_val: 98.92%\n","Epoch [ 52/60], Step [36/36], Loss: 0.0867, Accuracy: 96.78%\n","Epoch [ 52/60], Step_val [18/18], Loss_val: 0.0748, Accuracy_val: 97.90%\n","Epoch [ 53/60], Step [36/36], Loss: 0.2549, Accuracy: 97.09%\n","Epoch [ 53/60], Step_val [18/18], Loss_val: 0.1027, Accuracy_val: 97.62%\n","Epoch [ 54/60], Step [36/36], Loss: 0.2189, Accuracy: 96.47%\n","Epoch [ 54/60], Step_val [18/18], Loss_val: 0.0378, Accuracy_val: 98.24%\n","Epoch [ 55/60], Step [36/36], Loss: 0.0956, Accuracy: 96.92%\n","Epoch [ 55/60], Step_val [18/18], Loss_val: 0.4497, Accuracy_val: 85.73%\n","Epoch [ 56/60], Step [36/36], Loss: 0.0351, Accuracy: 96.19%\n","Epoch [ 56/60], Step_val [18/18], Loss_val: 0.0602, Accuracy_val: 97.90%\n","Epoch [ 57/60], Step [36/36], Loss: 0.0366, Accuracy: 97.04%\n","Epoch [ 57/60], Step_val [18/18], Loss_val: 0.0509, Accuracy_val: 97.06%\n","Epoch [ 58/60], Step [36/36], Loss: 0.1003, Accuracy: 97.85%\n","Epoch [ 58/60], Step_val [18/18], Loss_val: 0.0549, Accuracy_val: 98.47%\n","Epoch [ 59/60], Step [36/36], Loss: 0.2381, Accuracy: 97.40%\n","Epoch [ 59/60], Step_val [18/18], Loss_val: 0.0193, Accuracy_val: 99.38%\n","Epoch [ 60/60], Step [36/36], Loss: 0.1795, Accuracy: 96.89%\n","Epoch [ 60/60], Step_val [18/18], Loss_val: 0.0200, Accuracy_val: 99.15%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YwN0mMFOndq2","colab_type":"code","colab":{}},"source":["#Plotting loss vs epoch for training and validation:\n","loss_list_plot= [] \n","loss_list2_plot = []\n","epoch_plot = []\n","for i in range(len(loss_list)):\n","  if(i%2!=0):\n","    loss_list_plot.append(loss_list[i])\n","    loss_list2_plot.append(loss2_list[i])\n","    epoch_plot.append(i+2)\n","\n","\n","fig, ax = plt.subplots()\n","  \n","ax.plot(epoch_plot, loss_list_plot, '-b', label='Training' )\n","ax.plot(epoch_plot, loss_list2_plot, '--r', label='Validation')\n","plt.title(\"Loss Vs Epoch\")\n","plt.xlabel(\"Number of Epoch\")\n","plt.ylabel(\"Loss\")\n","leg = ax.legend();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xub8eUSqimf","colab_type":"code","colab":{}},"source":["#Plotting Accuracy vs Epoch for both valdiation and training \n","\n","acc_list_plot= [] \n","acc_list2_plot = []\n","\n","for i in range(len(acc_list)):\n","  if(i%2!=0):\n","    acc_list_plot.append(acc_list[i])\n","    accc_list2_plot.append(acc2_list[i])\n","\n","fig, ax = plt.subplots()\n","\n","ax.plot(epoch_plot, acc_list_plot,'-b', label='Training' )\n","ax.plot(epoch_plot, acc_list2_plot,  '-k', label='Test')\n","\n","\n","plt.title(\"Accuracy Vs Epoch\")\n","plt.xlabel(\"Number of Epoch\")\n","plt.ylabel(\"Accuracy\")\n","leg = ax.legend();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-edyvAEbRF93","colab_type":"code","colab":{}},"source":["predicted_train_real = []\n","labels_train_real = []\n","\n","predicted_test_real = []\n","labels_test_real = []\n","score_max = [] \n","\n","##Training: \n","model.eval()\n","with torch.no_grad():\n","    correct_train = 0\n","    total_train = 0\n","    for j, (images_train, labels_train) in enumerate(train_loader):\n","            images_train = images_train.to(device)\n","            labels_train = labels_train.to(device)\n","            outputs = model(images_train)\n","            _, predicted_train = torch.max(outputs.data, 1)\n","            total_train += labels_train.size(0)\n","            correct_train += (predicted_train == labels_train.long()).sum().item()\n","            predicted_train_real.append(predicted_train)\n","            labels_train_real.append(labels_train)\n","            \n","print('Training Accuracy of the model on the 3542 Training images: {} %'.format((correct_train / total_train) * 100))\n","\n","##Testing: \n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for j, (images_test, labels_test) in enumerate(test_loader):\n","            images_test = images_test.to(device)\n","            labels_test = labels_test.to(device)\n","            outputs2 = model(images_test)\n","            m = torch.nn.Softmax(dim=1)\n","            _, predicted_test = torch.max(outputs2.data, 1)\n","            total += labels_test.size(0)\n","            correct += (predicted_test == labels_test.long()).sum().item()\n","            predicted_test_real.append(predicted_test)\n","            labels_test_real.append(labels_test)\n","            score_max.append(torch.max(m(outputs2.data), 1))\n","            \n","            \n","\n","    print('Test Accuracy of the model on the 1776 Test images: {} %'.format((correct / total) * 100))\n","    predicted_test_real = torch.cat(predicted_test_real).long()\n","    labels_test_real = torch.cat(labels_test_real).long()\n","    score_max_real = []\n","  \n","    for i in range(18):\n","      score_max_real.append(score_max[i][0])\n","    \n","score_max_real = torch.cat(score_max_real)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tc7HloHG3YmU","colab_type":"code","colab":{}},"source":["print('confusion matrix of testing images')\n","print(confusion(labels_2000, predicted_2000))\n","\n","print('confusion matrix of training images')\n","print(confusion(labels_train_8000, predicted_train_8000))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxA_KNiiSpVn","colab_type":"code","colab":{}},"source":["from google.colab import files\n","uploaded = files.upload()\n","\n","import os\n","os.getcwd()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2y_qNuQVZmh","colab_type":"code","colab":{}},"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","img = cv2.imread(\"images.jpg\")\n","img_cvt=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.imshow(img_cvt)\n","plt.show()\n","img_cvt = torch.Tensor(img_cvt).unsqueeze(0)\n","\n","      \n","transform_train = transforms.Compose([\n","    transforms.ToTensor(),\n","    torchvision.transforms.ToPILImage(mode=None),\n","    torchvision.transforms.Resize((96,96), interpolation=2),\n","    transforms.ToTensor()\n","])\n","img_3d = transform_train(img)\n","print(img_3d.shape)\n","img = torch.reshape(torch.Tensor(img_3d),[1,3,96,96]).to(device)\n","\n","test = model(img)\n","_, predicted_valid = torch.max(test.data, 1)\n","prob = (torch.max(m(test.data), 1))\n","print(prob[0])\n","\n","if (predicted_valid == 0):\n","  print(\"Car\")\n","elif(predicted_valid == 1):\n","  print(\"aeroplane\")\n","elif(predicted_valid == 2):\n","  print(\"Bicycle\")\n","elif(predicted_valid == 3):\n","  print(\"Train\")\n","elif(predicted_valid == 4):\n","  print(\"Bus\")\n","elif(predicted_valid == 5):\n","  print(\"Motorbike\")\n","elif(predicted_valid == 6):\n","  print(\"Boat\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPAIJH09f69O","colab_type":"code","colab":{}},"source":["from matplotlib.patches import Rectangle\n","def Occlusion_exp(image, occluding_size, occluding_pixel, occluding_stride):\n","    _, height, width = image.shape\n","    print(height, width)\n","    output_height = int(math.ceil((height-occluding_size) / occluding_stride + 1))\n","    output_width = int(math.ceil((width-occluding_size) / occluding_stride + 1))\n","    heatmap = np.zeros((output_height, output_width))\n","    start_rec_h = []\n","    for h in range(output_height):\n","        for w in range(output_width):\n","            # Occluder region:\n","            h_start = h * occluding_stride\n","            w_start = w * occluding_stride\n","            h_end = min(height, h_start + occluding_size)\n","            w_end = min(width, w_start + occluding_size)\n","            # Getting the image copy, applying the occluding window and classifying it again:\n","            input_image = copy.copy(image)\n","            input_image[:,h_start:h_end, w_start:w_end] =  occluding_pixel\n","            \n","            im = np.expand_dims(input_image, axis=0)\n","            model.eval()\n","            with torch.no_grad():\n","              out = model(torch.from_numpy(im).float().to(device))\n","              prob = (torch.max(m(out.data),1))\n","              prob = (prob.cpu().data.numpy().item())\n","              print(prob)\n","              heatmap[h,w] = prob\n","              if prob>0.6 & (not start_rec_h):\n","                start_rec_w = w_start\n","                start_rec_h = h_start\n","                \n","    plt.gca().add_patch(Rectangle((start_rec_h,start_rec_w),40,30,linewidth=2,edgecolor='g',facecolor='none'))       \n","    return heatmap"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6KnDeXHDqVV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"7a8e0f98-0edf-4370-c238-24bf410b553b","executionInfo":{"status":"error","timestamp":1571547258684,"user_tz":-660,"elapsed":1270,"user":{"displayName":"Martin Tran","photoUrl":"","userId":"13848835499233746873"}}},"source":["\n","img = cv2.imread(\"images.jpg\")\n","img_cvt=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.imshow(img_cvt)\n","\n","heatmap = Occlusion_exp(img_3d,3,2,1)"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-29c87b1f325d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg_cvt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cvt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"]}]},{"cell_type":"code","metadata":{"id":"17cxpZ8MDpxO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}